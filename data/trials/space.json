{'batch_size': None, 'decay_steps': 10000.0, 'init': 'glorot_uniform', 'layers': {'BatchNormalization': True, 'activation': 'relu', 'dropout': 0.13705084927491545, 'next': {'BatchNormalization': False, 'activation': 'selu', 'dropout': 0.4045921460652302, 'next': None, 'nodes_count': 362}, 'nodes_count': 400}, 'optimizer': {'learning_rate': 0.6523162134375682, 'type': 'Adadelta'}, 'shuffle': True, 'trainable_BatchNormalization': True, 'trainable_dropouts': False}