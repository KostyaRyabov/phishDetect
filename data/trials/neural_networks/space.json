{'batch_size': 64, 'init': 'glorot_normal', 'layers': {'BatchNormalization': True, 'activation': 'relu', 'dropout': None, 'next': {'BatchNormalization': False, 'activation': 'relu', 'dropout': {'dropout_rate': 0.11666666666666667}, 'next': {'BatchNormalization': True, 'activation': 'exponential', 'dropout': {'dropout_rate': 0.15}, 'next': None, 'nodes_count': 5}, 'nodes_count': 140}, 'nodes_count': 100}, 'learning_rate': 0.0006, 'shuffle': True, 'trainable_BatchNormalization': True, 'trainable_dropouts': True}